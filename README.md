# NeRF
Implementation of NeRF

A high level overview  of the project. 

```bash
nerf_project/
â”œâ”€â”€ data/                   # raw and processed data
â”‚   â””â”€â”€ raw/                # your photos
â”‚   â””â”€â”€ colmap_output/      # output from COLMAP
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_colmap.sh       # runs COLMAP CLI steps
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ colmap_utils.py     # âœ… your functions (list, visualize, convert, etc.)
â”‚   â””â”€â”€ read_write_model.py # âœ… COLMAP binary reader
â”œâ”€â”€ dataset.py              # dataset loader (uses transforms.json)
â”œâ”€â”€ model.py                # NeRF model
â”œâ”€â”€ train.py                # training loop
â”œâ”€â”€ render.py               # volume rendering logic
â”œâ”€â”€ main.py                 # CLI or entrypoint
â””â”€â”€ README.md
```


# Diagrams for NeRF

```bash
nerf_minimal.py
â”œâ”€â”€ main()
â”‚   â”œâ”€â”€ get_rays()
â”‚   â”‚   â”œâ”€â”€ returns rays_o (origin)
â”‚   â”‚   â””â”€â”€ returns rays_d (direction)
â”‚   â”œâ”€â”€ sample_points_along_rays()
â”‚   â”‚   â””â”€â”€ uses rays_o, rays_d to create 3D sample points
â”‚   â”œâ”€â”€ positional_encoding()
â”‚   â”‚   â””â”€â”€ encodes 3D points into high-freq features
â”‚   â”œâ”€â”€ NeRF.forward()
â”‚   â”‚   â””â”€â”€ predicts [Ïƒ, R, G, B] for each 3D point
â”‚   â”œâ”€â”€ volume_rendering()
â”‚   â”‚   â”œâ”€â”€ computes alpha from Ïƒ
â”‚   â”‚   â”œâ”€â”€ blends RGB values using weights
â”‚   â”‚   â””â”€â”€ outputs final RGB pixel
â”‚   â”œâ”€â”€ compute_loss()
â”‚   â”‚   â””â”€â”€ compares pred_rgb vs target_rgb
â”‚   â””â”€â”€ optimizer.step()
â”‚       â””â”€â”€ updates model weights
```

## Interpretation of the tree

### Main
Main() is the trunk. Then we branch into 
* rays
* -> samples
* -> MLP
* -> volume rendering
* -> loss 


If you want to "walk" the data through the tree, you could: 
- Camera -> get_rays -> sample_points -> encode -> MLP -> render -> pixel 

This is a linear walk-through of the model.

## Task-Level Overview (Sequential Order)

```bash
NeRF Project: "House Plant"
â”œâ”€â”€ âœ… 1. Dataset Collection
â”‚   â””â”€â”€ Capture photos of house plant from multiple views
â”‚       (Done âœ…)
â”‚
â”œâ”€â”€ â³ 2. COLMAP Reconstruction
â”‚   â”œâ”€â”€ Run COLMAP to generate:
â”‚   â”‚   â”œâ”€â”€ camera poses (intrinsics + extrinsics)
â”‚   â”‚   â””â”€â”€ sparse/dense point cloud
â”‚   â”œâ”€â”€ Export data in NeRF-compatible format (e.g., transforms.json)
â”‚   â””â”€â”€ ğŸ“ Output: poses + images folder
â”‚
â”œâ”€â”€ ğŸ”² 3. Project Scaffolding
â”‚   â”œâ”€â”€ Setup repo: `nerf_project/`
â”‚   â”œâ”€â”€ Add `nerf_minimal.py` with placeholders
â”‚   â”œâ”€â”€ Organize file layout for modularity
â”‚   â””â”€â”€ Create `README.md` + config file
â”‚
â”œâ”€â”€ ğŸ§± 4. Core Modules (scaffold)
â”‚   â”œâ”€â”€ ray_sampling.py
â”‚   â”œâ”€â”€ positional_encoding.py
â”‚   â”œâ”€â”€ model.py (NeRF MLP)
â”‚   â”œâ”€â”€ render.py (volume renderer)
â”‚   â”œâ”€â”€ dataset.py (loads COLMAP + images)
â”‚   â””â”€â”€ train.py (training loop)
â”‚
â”œâ”€â”€ ğŸ§ª 5. Training & Evaluation
â”‚   â”œâ”€â”€ Train on house plant dataset
â”‚   â”œâ”€â”€ Save checkpoints
â”‚   â”œâ”€â”€ Evaluate PSNR vs ground truth
â”‚   â””â”€â”€ Visualize: render novel views
â”‚
â””â”€â”€ ğŸ¨ 6. Visualization & Export
    â”œâ”€â”€ Render a video rotating around plant
    â”œâ”€â”€ Compare input views vs synthesized views
    â””â”€â”€ Export images/video
```


### COLMAP Task Tree

```bash
COLMAP Reconstruction
â”œâ”€â”€ Install COLMAP
â”œâ”€â”€ Import images (JPEG or PNG)
â”œâ”€â”€ Run feature extraction
â”œâ”€â”€ Run matching (sequential or exhaustive)
â”œâ”€â”€ Run sparse reconstruction (SfM)
â”œâ”€â”€ Optionally run dense reconstruction
â”œâ”€â”€ Export camera poses
â”‚   â”œâ”€â”€ Convert to transforms.json (NeRF format)
â”‚   â””â”€â”€ Validate extrinsics match image order
â””â”€â”€ Output folder
    â”œâ”€â”€ images/
    â””â”€â”€ transforms.json (or equivalent)
```

#### Questions

-  